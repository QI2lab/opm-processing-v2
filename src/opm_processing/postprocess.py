"""
Command line tool for postprocessing qi2lab OPM data.

This script assumes data is generated by opm-v2 GUI and the resulting data is saved in zarr3 format.

By default, a max projection datastore is created for quick viewing.

History:
---------
- **2025/03**: Updated for new qi2lab OPM processing pipeline.
"""

from pathlib import Path
import tensorstore as ts
from opm_processing.imageprocessing.opmtools import deskew, deskew_shape_estimator
from opm_processing.imageprocessing.maxtilefusion import TileFusion
from opm_processing.dataio.metadata import extract_channels, find_key, extract_stage_positions, update_global_metadata, update_per_index_metadata
from opm_processing.dataio.zarr_handlers import create_via_tensorstore, write_via_tensorstore
import json
import numpy as np
from tqdm import tqdm
from basicpy import BaSiC
import typer

app = typer.Typer()
app.pretty_exceptions_enable = False

@app.command()
def postprocess(
    root_path: Path,
    max_projection: bool = True,
    flatfield_correction: bool = True,
    create_fused_max_projection: bool = True,
    display_fused_max_projection: bool = True,
    z_downsample_level: int = 2
):
    """Postprocess qi2lab OPM dataset.
    
    This code assumes data is generated by opm-v2 GUI and the resulting data is 
    saved using OPMMirrorHandler. All revelant metadata is read from imaging files, 
    including stage transformation, camera parameters, and channels. 
    
    Parameters
    ----------
    root_path: Path
        Path to OPM pymmcoregui zarr file.
    max_projection: bool, default = True
        Create a maximum projection datastore.
    flatfield_correction: bool, default = True
        Estimate and apply flatfield correction on raw data.
    display_max_projection: bool, default = False
        Display maximum projection in napari.
    display_full_volume: bool, default = False
        Display full volume in napari.

    z_downsample_level: int, default = 2
        Amount to downsample deskewed data in z.
    """
    
    # open raw datastore
    spec = {
        "driver" : "zarr",
        "kvstore" : {
            "driver" : "file",
            "path" : str(root_path)
        }
    }
    datastore = ts.open(spec).result()

    # Read metadata
    zattrs_path = root_path / Path(".zattrs")
    with open(zattrs_path, "r") as f:
        zattrs = json.load(f)

    image_mirror_step_um = float(find_key(zattrs,"image_mirror_step_um"))
    pixel_size_um = float(find_key(zattrs,"pixel_size_um"))
    opm_tilt_deg = float(find_key(zattrs,"angle_deg"))
    
    camera_offset = float(find_key(zattrs,"offset"))
    camera_conversion = float(find_key(zattrs,"e_to_ADU"))
    
    channels = extract_channels(zattrs)
    stage_positions = extract_stage_positions(zattrs)
    stage_y_flipped = True
    stage_z_flipped = True

    # flip y positions w.r.t. camera <-> stage orientation
    if stage_y_flipped:
        stage_y_max = np.max(stage_positions[:,1])
        for pos_idx, _ in enumerate(stage_positions):
            stage_positions[pos_idx,1] = stage_y_max - stage_positions[pos_idx,1]
    
    # flip z positions w.r.t. camera <-> stage orientation
    if stage_z_flipped:
        stage_z_max = np.max(stage_positions[:,0])
        for pos_idx, _ in enumerate(stage_positions):
            stage_positions[pos_idx,0] = stage_z_max - stage_positions[pos_idx,0]
    
    # # estimate shape of one deskewed volume
    deskewed_shape, pad_y, pad_x = deskew_shape_estimator(
        [datastore.shape[-3],datastore.shape[-2],datastore.shape[-1]],
        theta=opm_tilt_deg,
        distance=image_mirror_step_um,
        pixel_size=pixel_size_um
    )

    datastore_shape = [
        datastore.shape[0],
        datastore.shape[1],
        datastore.shape[2],
        deskewed_shape[0]//z_downsample_level,
        deskewed_shape[1],
        deskewed_shape[2]
    ]
    # create array to hold one deskewed volume 
    deskewed = np.zeros(
        (deskewed_shape[0]//z_downsample_level,deskewed_shape[1],deskewed_shape[2]),
        dtype=np.uint16
    )
    
    # create tensorstore object for writing. This is NOT compatible with OME-NGFF!
    output_path = root_path.parents[0] / Path(str(root_path.stem)+"_deskewed.zarr")
    ts_store = create_via_tensorstore(output_path,datastore_shape)

    if max_projection:
        max_z_datastore_shape = [
            datastore.shape[0],
            datastore.shape[1],
            datastore.shape[2],
            1,
            deskewed_shape[1],
            deskewed_shape[2]
        ]

        # create array to hold one maximum projection deskewed volume 
        max_z_deskewed = np.zeros(
            (1,deskewed_shape[1],deskewed_shape[2]),
            dtype=np.uint16
        )

        # create tensorstore object for writing. This is NOT compatible with OME-NGFF!
        max_z_output_path = root_path.parents[0] / Path(str(root_path.stem)+"_max_z_deskewed.zarr")
        max_z_ts_store = create_via_tensorstore(max_z_output_path,max_z_datastore_shape)
    
    if flatfield_correction:
        flatfields = np.zeros((datastore.shape[2],datastore.shape[-2],datastore.shape[-1]),dtype=np.float32)
        if datastore.shape[1] > 500:
            n_rand_images = 500
        else:
            n_rand_images = datastore.shape[1]
        sample_indices = list(np.random.choice(datastore.shape[1], size=n_rand_images, replace=False))
        for chan_idx in range(datastore.shape[2]):
            temp_images = ((np.squeeze(datastore[0,sample_indices,chan_idx,datastore.shape[-3]//2,:].read().result()).astype(np.float32)-camera_offset)*camera_conversion).clip(0,2**16-1).astype(np.uint16)
            basic = BaSiC(get_darkfield=False)
            basic.fit(temp_images)
            flatfields[chan_idx,:] = np.squeeze(basic.flatfield) / np.max(basic.flatfield,axis=(0,1))
        
    else:
        flatfields = np.ones((datastore.shape[2],datastore.shape[-2],datastore.shape[-1]),dtype=np.float32)
    
    # loop over all components and stream to zarr using tensorstore
    ts_writes = []
    if max_projection:
        ts_max_writes = []
    
    for t_idx in tqdm(range(datastore.shape[0]),desc="t"):
        for pos_idx in tqdm(range(datastore.shape[1]),desc="p",leave=False):
            for chan_idx in tqdm(range(datastore.shape[2]),desc="c",leave=False):
                camera_corrected_data = ((np.squeeze(datastore[t_idx,pos_idx,chan_idx,:].read().result()).astype(np.float32)-camera_offset)*camera_conversion)/(np.squeeze(flatfields[chan_idx,:])).clip(0,2**16-1).astype(np.uint16)
                deskewed = deskew(
                    camera_corrected_data,
                    theta = opm_tilt_deg,
                    distance = image_mirror_step_um,
                    pixel_size = pixel_size_um,
                )
              
                update_per_index_metadata(
                    ts_store = ts_store, 
                    metadata = {"stage_position": stage_positions[pos_idx], 'channel': channels[chan_idx]}, 
                    index_location = (t_idx,pos_idx,chan_idx)
                )
                
                if max_projection:
                    max_z_deskewed = np.max(deskewed,axis=0,keepdims=True)
                    update_per_index_metadata(
                        ts_store = max_z_ts_store, 
                        metadata = {"stage_position": stage_positions[pos_idx], 'channel': channels[chan_idx]}, 
                        index_location = (t_idx,pos_idx,chan_idx)
                    )
                    # create future objects for async data writing
                    ts_max_writes.append(
                        write_via_tensorstore(
                            ts_store = max_z_ts_store,
                            data = max_z_deskewed,
                            data_location = [t_idx,pos_idx,chan_idx]
                        )
                    )

                # create future objects for async data writing
                ts_writes.append(
                    write_via_tensorstore(
                        ts_store = ts_store,
                        data = deskewed,
                        data_location = [t_idx,pos_idx,chan_idx]
                    )
                )
                
    # wait for writes to finish
    for ts_write in tqdm(ts_writes,desc='writes'):
        ts_write.result()

    if max_projection:
        for ts_max_write in tqdm(ts_max_writes,desc='max writes'):
            ts_max_write.result()

    update_global_metadata(
        ts_store = ts_store,
        global_metadata= {
                "image_mirror_step_um" : image_mirror_step_um,
                "raw_pixel_size_um" : pixel_size_um,
                "opm_tilt_deg" : opm_tilt_deg,
                "camera_corrected" : True,
                "camera_offset" : camera_offset,
                "camera_e_to_ADU" : camera_conversion,
                "deskewed_voxel_size_um" : [z_downsample_level*pixel_size_um, pixel_size_um, pixel_size_um],
                "stage_y_flipped": stage_y_flipped,
                "stage_z_flipped": stage_z_flipped,
                "flatfield_corrected": flatfield_correction
            }
    )

    if max_projection:
        update_global_metadata(
            ts_store = max_z_ts_store,
            global_metadata= {
                "image_mirror_step_um" : image_mirror_step_um,
                "raw_pixel_size_um" : pixel_size_um,
                "opm_tilt_deg" : opm_tilt_deg,
                "camera_corrected" : True,
                "camera_offset" : camera_offset,
                "camera_e_to_ADU" : camera_conversion,
                "deskewed_voxel_size_um" : [pixel_size_um, pixel_size_um],
                "stage_y_flipped": stage_y_flipped,
                "stage_z_flipped": stage_z_flipped,
                "flatfield_corrected": flatfield_correction
            }
        )
        
    del deskewed, ts_write, ts_store
    if max_projection:
        del max_z_deskewed, ts_max_write
        
    if create_fused_max_projection:
      
        
        max_z_output_path = root_path.parents[0] / Path(str(root_path.stem)+"_max_z_deskewed.zarr")
        # open datastore on disk
        spec = {
            "driver" : "zarr3",
            "kvstore" : {
                "driver" : "file",
                "path" : str(max_z_output_path)
            }
        }
        max_z_ts_store = ts.open(spec).result()
        
        print("\nEstimating flatfields...")
        max_flatfields = np.zeros((max_z_ts_store.shape[2],max_z_ts_store.shape[-2],max_z_ts_store.shape[-1]),dtype=np.float32)
        if max_z_ts_store.shape[1] > 500:
            n_rand_images = 500
        else:
            n_rand_images = max_z_ts_store.shape[1]
        sample_indices = list(np.random.choice(max_z_ts_store.shape[1], size=n_rand_images, replace=False))
        for chan_idx in tqdm(range(max_z_ts_store.shape[2]),desc='chan'):
            temp_images = np.squeeze(max_z_ts_store[0,sample_indices,chan_idx,:].read().result()).astype(np.float32)
            basic = BaSiC(get_darkfield=False)
            #basic.autotune(temp_images, early_stop=True, n_iter=100)
            basic.fit(temp_images)
            max_flatfields[chan_idx,:] = np.squeeze(basic.flatfield) / np.max(np.squeeze(basic.flatfield),axis=(0,1))
        
        print("\nFusing using stage positions...")
        fused_output_path = root_path.parents[0] / Path(str(root_path.stem)+"_max_zfused.zarr")
        tile_fusion = TileFusion(
            ts_dataset = max_z_ts_store,
            tile_positions = stage_positions[:,1:],
            output_path=fused_output_path,
            pixel_size=np.asarray((pixel_size_um,pixel_size_um),dtype=np.float32),
            flatfields = max_flatfields
        )
        tile_fusion.run()
        
        
    if display_fused_max_projection:

        from cmap import Colormap
        import napari
        
        fused_output_path = root_path.parents[0] / Path(str(root_path.stem)+"_max_zfused.zarr")
        # open datastore on disk
        spec = {
            "driver" : "zarr3",
            "kvstore" : {
                "driver" : "file",
                "path" : str(fused_output_path)
            }
        }
        fused_max_z_ts_store = ts.open(spec).result()
        

        
        colormaps = [
            Colormap("chrisluts:bop_purple").to_napari(),
            Colormap("chrisluts:bop_blue").to_napari(),
            Colormap("chrisluts:bop_orange").to_napari(),
        ]
        viewer = napari.Viewer()
        for time_idx in range(fused_max_z_ts_store.shape[0]):
            for pos_idx in range(fused_max_z_ts_store.shape[1]):
                for chan_idx in range(fused_max_z_ts_store.shape[2]):
                    viewer.add_image(
                        np.squeeze((fused_max_z_ts_store[time_idx,pos_idx,chan_idx,:]).read().result()),
                        scale=[pixel_size_um,pixel_size_um],
                        name = "c"+str(chan_idx).zfill(2),
                        blending="additive",
                        colormap=colormaps[chan_idx],
                        contrast_limits = [0,500]
                    )
        napari.run()

# entry for point for CLI        
def main():
    app()

if __name__ == "__main__":
    main()